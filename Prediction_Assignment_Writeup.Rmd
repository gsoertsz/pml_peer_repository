---
title: "Predicting exercise type using personal activity data"
author: "Greg Soertsz"
date: "26-10-2016"
output: html_document
---

# Introduction

This report outlines an exercise to train a classifier to predict the _type_ of exercise undertaken by a group of study participants based on data collected from a number of sensors positioned in various places on their body. Using data from the Human Activity Recognition study documented [here](http://groupware.les.inf.puc-rio.br/har), 

# Executive Summary

A classifier was trained to identify the type of activity given a number of input measures. A randomForest training algorithm was applied, using a resampling approach based on repeated cross validation based on training data of a chosen strata, and column projection.

The trained models were evaluated using overall accuracy and ROC performance. Strong accuracy results were achieved for class A and B, with moderate-to-good accuracy results achieved for the other classes. Based on the overall accuracies, out-of-sample errors, and visualised ROC outputs, the training method can be summarized as follows:

* Random Forest training algorithm
* Initial Training Strata = 7/8, specific columns selected (excluded time-based columns)
* Resampling
  * Repeated cross validation
  * 5 folds
  * 1 Repeat
* Evaluation
  * AUC accuracy = 0.995
  * Out of sample error estimate = 0.4%
  
```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
library(RCurl)
library(e1071)
library(randomForest)
library(caret)
library(dplyr)
library(pROC)
library(xtable)

library(GGally)
library(gridExtra)
```

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}

readAndMaybeDownloadData <- function(remoteUrl, parentDirectory = getwd(), localFile, force=FALSE) {
    localFilePath <- paste(parentDirectory, "data", localFile, sep="/")
    parent = paste(parentDirectory, "data", sep="/")
    if (file.exists(localFilePath)) {
      if (force) {
        unlink(parent, recursive=TRUE, force=TRUE);
        dir.create(parent);
        download.file(url = remoteUrl, destfile = localFilePath, method = "curl"); 
      }
    } else {
      dir.create(parent);
      download.file(url = remoteUrl, destfile = localFilePath, method = "curl"); 
    }

    read.csv(file = localFilePath, header = TRUE);
}

PARENT_DIRECTORY = "/Users/gsoertsz/Coursera/DataScience/PracticalMachineLearning/Assignments"

raw_training <- readAndMaybeDownloadData("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", parentDirectory = PARENT_DIRECTORY, "pml-training.csv")
raw_testing <- readAndMaybeDownloadData("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", parentDirectory = PARENT_DIRECTORY, "pml-testing.csv")
```

# Selecting columns

Upon initial inspection of the training data, a specific subset of the columns should be used, to avoid empty or NULL values, or summary (duplicated) data.

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}

relevantColumns <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 'gyros_belt_x', 'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x', 'accel_belt_y', 'accel_belt_z', 'magnet_belt_x', 'magnet_belt_y', 'magnet_belt_z', 'magnet_dumbbell_x', 'magnet_dumbbell_y', 'magnet_dumbbell_z', 'roll_forearm', 'pitch_forearm', 'yaw_forearm', 'total_accel_forearm', 'gyros_forearm_x', 'gyros_forearm_y', 'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y', 'accel_forearm_z', 'magnet_forearm_x', 'magnet_forearm_y', 'magnet_forearm_z', 'classe')

# remove timestamp and other meta columns
relevantTrainingColumns <- relevantColumns[-c(1, 2, 3, 4, 5)]

base_training <- mutate(raw_training[, relevantTrainingColumns], classe = as.factor(classe))
```

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
evaluate <- function(cross_validation, probs) {
  
  roc_a <- roc(predictor=probs$A, response=cross_validation$classe)
  roc_b <- roc(predictor=probs$B, response=cross_validation$classe)
  roc_c <- roc(predictor=probs$C, response=cross_validation$classe)
  roc_d <- roc(predictor=probs$D, response=cross_validation$classe)
  roc_e <- roc(predictor=probs$E, response=cross_validation$classe) 
  roc_viz_a <- data.frame(sensitivity = roc_a$sensitivities, specificity = roc_a$specificities)
  roc_viz_b <- data.frame(sensitivity = roc_b$sensitivities, specificity = roc_b$specificities)
  roc_viz_c <- data.frame(sensitivity = roc_c$sensitivities, specificity = roc_c$specificities)
  roc_viz_d <- data.frame(sensitivity = roc_d$sensitivities, specificity = roc_d$specificities)
  roc_viz_e <- data.frame(sensitivity = roc_e$sensitivities, specificity = roc_e$specificities)
  baseline <- data.frame(sensitivity = c(0.0, 1.0), specificity = c(0.0, 1.0))
  
  g <- ggplot() + geom_line(data = roc_viz_a, aes(x = 1 - specificity, y = sensitivity, color="red"))
  g <- g + geom_line(data = roc_viz_b, aes(x = 1 - specificity, y = sensitivity, color="blue"))
  g <- g + geom_line(data = roc_viz_c, aes(x = 1 - specificity, y = sensitivity, color="green"))
  g <- g + geom_line(data = roc_viz_d, aes(x = 1 - specificity, y = sensitivity, color="orange"))
  g <- g + geom_line(data = roc_viz_e, aes(x = 1 - specificity, y = sensitivity))
  g <- g + geom_line(data = baseline, aes(x = specificity, y = sensitivity), linetype="dotted")
  g <- g + ggtitle("Classe Classifier ROC Performance")
  g <- g + theme(legend.position="none")
  g
}

require(compiler)
multiClassSummary <- cmpfun(function (data, lev = NULL, model = NULL){
  
  #Load Libraries
  require(Metrics)
  require(caret)
  
  #Check data
  if (!all(levels(data[, "pred"]) == levels(data[, "obs"]))) 
    stop("levels of observed and predicted data do not match")
  
  #Calculate custom one-vs-all stats for each class
  prob_stats <- lapply(levels(data[, "pred"]), function(class){
    
    #Grab one-vs-all data for the class
    pred <- ifelse(data[, "pred"] == class, 1, 0)
    obs  <- ifelse(data[,  "obs"] == class, 1, 0)
    prob <- data[,class]

    #Calculate one-vs-all AUC and logLoss and return
    cap_prob <- pmin(pmax(prob, .000001), .999999)
    prob_stats <- c(auc(obs, prob), logLoss(obs, cap_prob))
    names(prob_stats) <- c('ROC', 'logLoss')
    return(prob_stats) 
  })
  prob_stats <- do.call(rbind, prob_stats)
  rownames(prob_stats) <- paste('Class:', levels(data[, "pred"]))
  
  #Calculate confusion matrix-based statistics
  CM <- confusionMatrix(data[, "pred"], data[, "obs"])
  
  #Aggregate and average class-wise stats
  #Todo: add weights
  class_stats <- cbind(CM$byClass, prob_stats)
  class_stats <- colMeans(class_stats)

  #Aggregate overall stats
  overall_stats <- c(CM$overall)
 
  #Combine overall with class-wise stats and remove some stats we don't want 
  stats <- c(overall_stats, class_stats)
  stats <- stats[! names(stats) %in% c('AccuracyNull', 
    'Prevalence', 'Detection Prevalence')]
  
  #Clean names and return
  names(stats) <- gsub('[[:blank:]]+', '_', names(stats))
  return(stats)
  
})
```

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
set.seed(64433)

inTrain <- createDataPartition(base_training$classe, p = 3/4)[[1]]
training <- base_training[inTrain, ]
cross_validation <- base_training[-inTrain, ]
```

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
set.seed(33434)

inTrain1 <- createDataPartition(base_training$classe, p = 1/2)[[1]]
training1 <- base_training[inTrain1, ]
cross_validation1 <- base_training[-inTrain1, ]

inTrain2 <- createDataPartition(base_training$classe, p = 3/4)[[1]]
training2 <- base_training[inTrain2, ]
cross_validation2 <- base_training[-inTrain2, ]

inTrain3 <- createDataPartition(base_training$classe, p = 7/8)[[1]]
training3 <- base_training[inTrain3, ]
cross_validation3 <- base_training[-inTrain3, ]

inTrain4 <- createDataPartition(base_training$classe, p = 15/16)[[1]]
training4 <- base_training[inTrain4, ]
cross_validation4 <- base_training[-inTrain4, ]

tc <- trainControl(method="repeatedcv", number=5, repeats=1, classProbs=TRUE, summaryFunction=multiClassSummary)
```

```{r eval=TRUE, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
fit_rf1 <- train(classe ~ ., data = training1, trControl=tc, method="rf", metric="ROC")
result_rf.1.probs <- predict(fit_rf1, newdata=cross_validation1, type="prob")
result_rf.1.base <- predict(fit_rf1, newdata=cross_validation1, type="raw")
g.rf.1 <- evaluate(cross_validation1, probs=result_rf.1.probs)
c.rf.1 <- confusionMatrix(result_rf.1.base, cross_validation1$classe)
c.acc.1 <- c.rf.1$overall[[1]]
ooe.1 <- tail(data.frame(fit_rf1$finalModel$err.rate), 1)$OOB
```

```{r eval=T, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
fit_rf2 <- train(classe ~ ., data = training2, trControl=tc, method="rf", metric="ROC")
result_rf.2.probs <- predict(fit_rf2, newdata=cross_validation2, type="prob")
result_rf.2.base <- predict(fit_rf2, newdata=cross_validation2, type="raw")
g.rf.2 <- evaluate(cross_validation2, probs=result_rf.2.probs)
c.rf.2 <- confusionMatrix(result_rf.2.base, cross_validation2$classe)
c.acc.2 <- c.rf.2$overall[[1]]
ooe.2 <- tail(data.frame(fit_rf2$finalModel$err.rate), 1)$OOB
```

```{r eval=T, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
fit_rf3 <- train(classe ~ ., data = training3, trControl=tc, method="rf", metric="ROC")
result_rf.3.probs <- predict(fit_rf3, newdata=cross_validation3, type="prob")
result_rf.3.base <- predict(fit_rf3, newdata=cross_validation3, type="raw")
g.rf.3 <- evaluate(cross_validation3, probs=result_rf.3.probs)
c.rf.3 <- confusionMatrix(result_rf.3.base, cross_validation3$classe)
c.acc.3 <- c.rf.3$overall[[1]]
ooe.3 <- tail(data.frame(fit_rf3$finalModel$err.rate), 1)$OOB
```

```{r eval=T, echo=F, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
fit_rf4 <- train(classe ~ ., data = training4, trControl=tc, method="rf", metric="ROC")
result_rf.4.probs <- predict(fit_rf4, newdata=cross_validation4, type="prob")
result_rf.4.base <- predict(fit_rf4, newdata=cross_validation4, type="raw")
g.rf.4 <- evaluate(cross_validation4, probs=result_rf.4.probs)
c.rf.4 <- confusionMatrix(result_rf.4.base, cross_validation4$classe)
c.acc.4 <- c.rf.4$overall[[1]]
ooe.4 <- tail(data.frame(fit_rf4$finalModel$err.rate), 1)$OOB
```

```{r eval=F, echo=TRUE, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
grid.arrange(g.rf.1, ncol=2, nrow=2)
```

```{r eval=T, echo=TRUE, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
grid.arrange(g.rf.1, g.rf.2, g.rf.3, g.rf.4, ncol=2, nrow=2)

bwplot(resamples(list(RF1 = fit_rf1, RF2 = fit_rf2, RF3 = fit_rf3, RF4 = fit_rf4)), metric=c("ROC", "Specificity", "Sensitivity"))
```

```{r eval=T, echo=TRUE, results='asis', message=FALSE, error=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=60)}
strata <- c(1/2, 3/4, 7/8, 15/16)
folds <- c(5, 5, 5, 5)
repeats <- c(1, 1, 1, 1)
accuracies <- c(c.acc.1, c.acc.2, c.acc.3, c.acc.4)
oob <- c(ooe.1, ooe.2, ooe.3, ooe.4)

acc <- data.frame(Strata = strata, Accuracy = accuracies, OOB = oob)

print.xtable(xtable(acc), type="html")
```


